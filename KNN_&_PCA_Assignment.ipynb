{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7WVeL13Yi6c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN & PCA | Assignment"
      ],
      "metadata": {
        "id": "JFighMz7Y3uP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "Ans. K-Nearest Neighbors (KNN):\n",
        "\n",
        " KNN is a supervised learning algorithm used for classification and regression.\n",
        " It’s a lazy learner (no explicit training phase) and non-parametric (makes no assumptions about data distribution).\n",
        "\n",
        "How it works (General Idea):\n",
        "\n",
        "Choose a number K (the number of neighbors).\n",
        "\n",
        "For a new data point:\n",
        "\n",
        "Calculate the distance (commonly Euclidean) between the new point and all points in the training data.\n",
        "\n",
        "Identify the K closest data points (neighbors).\n",
        "\n",
        "Predict the output based on those neighbors.\n",
        "\n",
        "In Classification:\n",
        "\n",
        "Each neighbor \"votes\" for its class.\n",
        "\n",
        "The class with the majority votes becomes the predicted class.\n",
        "\n",
        "👉 Example:\n",
        "If K=5 and neighbors’ classes are [A, A, B, A, B] → Majority is A, so prediction = A.\n",
        "\n",
        "In Regression:\n",
        "\n",
        "Instead of voting, take the average (or weighted average) of the neighbors’ values.\n",
        "\n",
        "👉 Example:\n",
        "If K=3 and neighbors’ target values are [10, 12, 14] → Prediction = (10+12+14)/3 = 12.\n",
        "\n",
        "Key Points about KNN:\n",
        "\n",
        "Distance Metrics: Euclidean, Manhattan, Minkowski, Cosine similarity.\n",
        "\n",
        "Choice of K:\n",
        "\n",
        "Small K → sensitive to noise (overfitting).\n",
        "\n",
        "Large K → smoother decision boundary but may underfit.\n",
        "\n",
        "Feature Scaling: Very important (since distances dominate) → use normalization/standardization\n",
        "\n",
        "\n",
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "Ans.  -  Curse of Dimensionality\n",
        "\n",
        "The curse of dimensionality refers to the problems that arise when data has too many features (dimensions).\n",
        "As dimensions increase:\n",
        "\n",
        "Data becomes sparse (spread out).\n",
        "\n",
        "Distances between points become less meaningful.\n",
        "\n",
        "Models relying on distance similarity (like KNN) start to perform poorly.\n",
        "\n",
        "- How It Affects KNN Performance\n",
        "\n",
        "Distance Becomes Less Discriminative\n",
        "\n",
        "In high dimensions, the difference between the nearest and farthest neighbor shrinks.\n",
        "\n",
        "All points appear almost equally distant → KNN can’t distinguish neighbors well.\n",
        "\n",
        "Increased Computation\n",
        "\n",
        "KNN requires computing distances to all training points.\n",
        "\n",
        "As dimensions grow, computation becomes expensive.\n",
        "\n",
        "Overfitting Risk\n",
        "\n",
        "With many irrelevant features, KNN may consider noisy dimensions in distance calculation.\n",
        "\n",
        "This misleads the algorithm → poor generalization.\n",
        "\n",
        "- Example:\n",
        "\n",
        "Imagine classifying points in:\n",
        "\n",
        "2D (x,y): You can easily find \"close neighbors.\"\n",
        "\n",
        "100D: Almost every point is far away → \"nearest\" loses meaning.\n",
        "\n",
        "- How to Reduce Curse of Dimensionality in KNN\n",
        "\n",
        "Feature Selection: Keep only important features.\n",
        "\n",
        "Dimensionality Reduction: Use PCA, t-SNE, Autoencoders.\n",
        "\n",
        "Scaling/Normalization: Ensures no feature dominates distance.\n",
        "\n",
        "Use weighted distances: Closer neighbors get higher weight.\n",
        "\n",
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "Ans.Principal Component Analysis (PCA)\n",
        "\n",
        "PCA is a dimensionality reduction technique used in machine learning and statistics.\n",
        "It transforms high-dimensional data into a new set of features (called principal components) that capture the maximum variance in the data.\n",
        "\n",
        "- How PCA Works (Steps):\n",
        "\n",
        "Standardize the data (so all features are on the same scale).\n",
        "\n",
        "Compute the covariance matrix to understand feature relationships.\n",
        "\n",
        "Find eigenvalues & eigenvectors of the covariance matrix.\n",
        "\n",
        "Eigenvectors = directions of new features (principal components).\n",
        "\n",
        "Eigenvalues = how much variance each component explains.\n",
        "\n",
        "Select top k components that explain most of the variance.\n",
        "\n",
        "Transform original data into this new reduced feature space.\n",
        "\n",
        "- Example: If you have 100 features, PCA may reduce them to 10 principal components while still keeping ~90% of the variance.\n",
        "\n",
        "- Principal Component Analysis (PCA) and feature selection are both dimensionality reduction techniques, but they work differently. PCA is a feature extraction method that transforms the original features into new ones called principal components, which are linear combinations of the existing features and capture the maximum variance in the data. In contrast, feature selection does not create new features but instead identifies and retains only the most relevant original features, removing the less useful or redundant ones. While PCA often improves performance by eliminating correlations and reducing noise, it reduces interpretability since the new components are not the same as the original features. Feature selection, on the other hand, maintains interpretability because it directly works with the original feature set.\n",
        "\n",
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Ans. Eigenvalues and Eigenvectors in PCA\n",
        "\n",
        "Eigenvectors:\n",
        "These represent the directions of the new feature space (principal components).\n",
        "Each eigenvector points in the direction where data varies the most.\n",
        "\n",
        "Eigenvalues:\n",
        "These represent the amount of variance captured by their corresponding eigenvectors.\n",
        "Larger eigenvalue → more information (variance) that component holds.\n",
        "\n",
        "- Why They Are Important in PCA\n",
        "\n",
        "Determine Principal Components\n",
        "\n",
        "PCA computes eigenvectors of the covariance matrix of the data.\n",
        "\n",
        "Each eigenvector is a principal component direction.\n",
        "\n",
        "Rank Components by Importance\n",
        "\n",
        "Eigenvalues tell us how much variance each component explains.\n",
        "\n",
        "- Example: If PC1’s eigenvalue = 5 and PC2’s eigenvalue = 2, then PC1 captures more variance.\n",
        "\n",
        "Dimensionality Reduction\n",
        "\n",
        "We keep only the top k eigenvectors (with the largest eigenvalues).\n",
        "\n",
        "This way, we reduce dimensions while keeping most of the variance.\n",
        "\n",
        "Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "Ans. How KNN and PCA Complement Each Other\n",
        "\n",
        "1.KNN struggles in high dimensions\n",
        "\n",
        "KNN relies on distance calculations.\n",
        "\n",
        "In high-dimensional spaces (curse of dimensionality), distances lose meaning → performance drops.\n",
        "\n",
        "2.PCA reduces dimensions before KNN\n",
        "\n",
        "PCA transforms the data into a smaller set of uncorrelated features (principal components).\n",
        "\n",
        "This removes noise and redundancy, making distance calculations more reliable.\n",
        "\n",
        "3.Improved Efficiency\n",
        "\n",
        "With fewer dimensions, KNN computes distances much faster.\n",
        "\n",
        "This is important since KNN has high prediction-time cost.\n",
        "\n",
        "4.Better Generalization\n",
        "\n",
        "By keeping only the top principal components, PCA reduces overfitting.\n",
        "\n",
        "KNN then focuses on the most informative features, improving accuracy.\n",
        "\n",
        "- In short:\n",
        "\n",
        "When combined, PCA reduces data complexity and noise, and KNN uses the cleaner, lower-dimensional space to make more accurate distance-based predictions."
      ],
      "metadata": {
        "id": "m4mPhN_7YwPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6:Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n"
      ],
      "metadata": {
        "id": "srdwq-UocHlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ------------------ Without Feature Scaling ------------------\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# ------------------ With Feature Scaling ------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = knn_scaling.predict(X_test_scaled)\n",
        "acc_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy without scaling:\", acc_no_scaling)\n",
        "print(\"Accuracy with scaling   :\", acc_scaling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e9HIKMhcMMY",
        "outputId": "c6e57650-545b-4f5f-bb72-947242bae5aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407407407407407\n",
            "Accuracy with scaling   : 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected Result\n",
        "\n",
        "Without scaling: Accuracy is usually much lower (around ~0.65–0.75).\n",
        "\n",
        "With scaling: Accuracy improves significantly (often 0.95+).\n",
        "\n",
        "- Conclusion: Feature scaling is crucial for KNN because it ensures that all features contribute equally to the distance metric."
      ],
      "metadata": {
        "id": "XG2o177wcZah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component."
      ],
      "metadata": {
        "id": "jb3pBSjucf3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Standardize features (important before PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "print(\"Explained Variance Ratio of each Principal Component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4llvbBfyck7S",
        "outputId": "33a67ffc-397e-4593-b5c8-c566ef807773"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this does:\n",
        "\n",
        "explained_variance_ratio_ → fraction of variance explained by each principal component.\n",
        "\n",
        "The values add up to 1 (100%).\n",
        "\n",
        "The first few PCs usually capture most of the variance (e.g., PC1 + PC2 may explain ~60–70%).\n",
        "\n",
        "- Conclusion: This output tells you how many components you need to keep while still preserving most of the dataset’s information."
      ],
      "metadata": {
        "id": "Am9JgzoVcwzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.\n"
      ],
      "metadata": {
        "id": "Dhyb536Yc2w4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Standardize features (important for PCA & KNN)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# ------------------ KNN on Original Data ------------------\n",
        "knn_orig = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_orig.fit(X_train, y_train)\n",
        "y_pred_orig = knn_orig.predict(X_test)\n",
        "acc_orig = accuracy_score(y_test, y_pred_orig)\n",
        "\n",
        "# ------------------ PCA Transformation (Top 2 Components) ------------------\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(\n",
        "    X_pca, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train_pca)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test_pca, y_pred_pca)\n",
        "\n",
        "# ------------------ Print Results ------------------\n",
        "print(\"Accuracy on Original Dataset :\", acc_orig)\n",
        "print(\"Accuracy on PCA (2 components):\", acc_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn2BAVt9c36_",
        "outputId": "c3cc58d7-fab2-4b0a-ad1a-4ca24af1200d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Original Dataset : 0.9629629629629629\n",
            "Accuracy on PCA (2 components): 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n"
      ],
      "metadata": {
        "id": "xYXVcnVzdFqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Standardize features (important for KNN)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# ------------------ KNN with Euclidean Distance ------------------\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric=\"euclidean\")\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# ------------------ KNN with Manhattan Distance ------------------\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric=\"manhattan\")\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# ------------------ Print Results ------------------\n",
        "print(\"Accuracy with Euclidean Distance:\", acc_euclidean)\n",
        "print(\"Accuracy with Manhattan Distance:\", acc_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_28b-QBOdGvo",
        "outputId": "cdbcda5a-e2c1-451d-876a-88b1e4e4b965"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean Distance: 0.9629629629629629\n",
            "Accuracy with Manhattan Distance: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "Ans.  Problem Context\n",
        "\n",
        "Dataset: High-dimensional gene expression data (thousands of genes = features).\n",
        "\n",
        "Samples: Relatively few patient cases (small n, large p problem).\n",
        "\n",
        "Challenge: Models tend to overfit due to high dimensionality and noise.\n",
        "\n",
        "- 1 Use PCA to Reduce Dimensionality\n",
        "\n",
        "Apply feature scaling first (Standardization).\n",
        "\n",
        "Run PCA on the scaled dataset.\n",
        "\n",
        "PCA will transform thousands of gene features into a smaller set of principal components (PCs) that capture the majority of variance (biological signal).\n",
        "\n",
        "This reduces noise, eliminates redundancy, and makes the data more manageable.\n",
        "\n",
        "- 2 Decide How Many Components to Keep\n",
        "\n",
        "Look at the explained variance ratio from PCA.\n",
        "\n",
        "Plot a cumulative variance curve (scree plot).\n",
        "\n",
        "Choose the smallest number of PCs that explain, say, 90–95% of the variance.\n",
        "\n",
        "Example: From 10,000 genes → maybe only 50–100 PCs are enough.\n",
        "\n",
        "- 3 Use KNN for Classification (Post-PCA)\n",
        "\n",
        "Train a KNN classifier on the PCA-transformed dataset.\n",
        "\n",
        "Use grid search with cross-validation to tune hyperparameters:\n",
        "\n",
        "k (number of neighbors).\n",
        "\n",
        "Distance metric (Euclidean, Manhattan).\n",
        "\n",
        "Since PCA removed noise and correlated features, distance measures in KNN are now more reliable.\n",
        "\n",
        "- 4 Evaluate the Model\n",
        "\n",
        "Use stratified cross-validation (important with small samples).\n",
        "\n",
        "Metrics: Accuracy, Precision, Recall, F1-score (since misclassification in medical settings has high cost).\n",
        "\n",
        "Compare results with and without PCA to demonstrate improvements in generalization.\n",
        "\n",
        "- 5 Justification to Stakeholders\n",
        "\n",
        "Why PCA?\n",
        "Gene expression datasets are high-dimensional and noisy. PCA compresses data into fewer, biologically meaningful components, reducing overfitting risk.\n",
        "\n",
        "Why KNN?\n",
        "KNN is a simple, interpretable, and effective algorithm when features are reduced and scaled. It doesn’t assume linearity, which is important for complex biological patterns.\n",
        "\n",
        "Why this pipeline is robust?\n",
        "\n",
        "Handles curse of dimensionality.\n",
        "\n",
        "Prevents overfitting by keeping only the most informative signals.\n",
        "\n",
        "Computationally efficient (fewer features).\n",
        "\n",
        "Transparent: PCA variance ratios and KNN neighborhood decisions can be communicated to clinicians.\n",
        "\n",
        "- Summary in one line:\n",
        "We reduce thousands of gene features into a small set of principal components with PCA, train a tuned KNN classifier on this lower-dimensional space, and evaluate with cross-validation — ensuring a robust, interpretable, and generalizable cancer classification model."
      ],
      "metadata": {
        "id": "mHSYsowmdTRP"
      }
    }
  ]
}